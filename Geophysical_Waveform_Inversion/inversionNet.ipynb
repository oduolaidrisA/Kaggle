{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f205062a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import seed_everything\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.set_float32_matmul_precision('high') # or'high'. This is to properly utilize Tensor Cores of my CUDA device ('NVIDIA RTX A6000')\n",
    "# import multiprocessing\n",
    "# To set start method safely (only once, at top of script/notebook)\n",
    "# multiprocessing.set_start_method('spawn', force=True)\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9c56a",
   "metadata": {},
   "source": [
    "## 1. Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6db9d012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\idris_oduola\\Documents\\Projects\\FWI\\dataset\\train_samples\\FlatFault_B\\seis6_1_0.npy\n",
      "C:\\Users\\idris_oduola\\Documents\\Projects\\FWI\\dataset\\train_samples\\FlatFault_B\\seis8_1_0.npy\n",
      "C:\\Users\\idris_oduola\\Documents\\Projects\\FWI\\dataset\\train_samples\\FlatFault_B\\vel6_1_0.npy\n",
      "C:\\Users\\idris_oduola\\Documents\\Projects\\FWI\\dataset\\train_samples\\FlatFault_B\\vel8_1_0.npy\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"C:/Users/idris_oduola/Documents/Projects/FWI/dataset/train_samples/FlatFault_B\")\n",
    "for file in data_dir.iterdir():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7db6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 13\n"
     ]
    }
   ],
   "source": [
    "folder = Path(\"C:/Users/idris_oduola/Documents/Projects/FWI/dataset/test\")\n",
    "file_count = sum(1 for f in folder.iterdir() if f.is_file())\n",
    "print(f\"Total files: {file_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808f1a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of files in test data is: 13\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        #The File paths\n",
    "        self.train_path = 'C:/Users/idris_oduola/Documents/Projects/FWI/dataset/train_samples'\n",
    "        self.test_path = 'C:/Users/idris_oduola/Documents/Projects/FWI/dataset/test'\n",
    "        self.test_num = sum(1 for f in folder.iterdir() if f.is_file())\n",
    "        print(f'The number of files in test data is: {self.test_num}') \n",
    "        #self.submission_path = 'C:/Users/idris_oduola/Documents/Projects/FWI/sample_submission.csv'\n",
    "        self.model_path = 'C:/Users/idris_oduola/Documents/Projects/FWI/dataset/fwi_model.pt'\n",
    "        self.checkpoint_dir = 'C:/Users/idris_oduola/Documents/Projects/FWI/dataset/checkpoint_fwi'\n",
    "        self.dset1 = [\"FlatVel_A\",\"FlatVel_B\", \"Style_A\", \"Style_B\",\"CurveVel_A\",\"CurveVel_B\"] #Dataset storage names used for training\n",
    "        self.dset2 = [\"FlatFault_A\", \"FlatFault_B\", \"CurveFault_A\"]\n",
    "\n",
    "        #Model Parameters\n",
    "        self.init_channels = 5\n",
    "        self.final_channel = 1\n",
    "        self.depth = 4\n",
    "        self.base_channel = 64\n",
    "        \n",
    "        \n",
    "        #Optimizer\n",
    "        self.lr = 0.0005\n",
    "        self.weight_decay = 1e-4 #Regularization weight\n",
    "        \n",
    "        #The training parameters\n",
    "        self.num_epoch = 30\n",
    "        self.batch_size = 50\n",
    "\n",
    "        #Learning rate scheduler\n",
    "        self.step_size = 15  #To decay after every, say 10 epochs\n",
    "        self.gamma = 0.1      #To reduce the learning rate by gamma (say, 1/2)\n",
    "\n",
    "        \n",
    "\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f62d5",
   "metadata": {},
   "source": [
    "### 1.1 Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d51aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch Dataset for DataLoader, we set velocity initially to None in the case of loading the test set\n",
    "#Note: We can also use the TensorDataset from torch.utils.data\n",
    "class SeismicDataset(Dataset):\n",
    "    def __init__(self, seismic, vel = None):\n",
    "        self.seismic = torch.tensor(seismic, dtype = torch.float32)\n",
    "        self.label = vel is not None\n",
    "        if self.label:\n",
    "            self.vel = torch.tensor(vel, dtype = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seismic)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        if self.label:\n",
    "            return self.seismic[idx], self.vel[idx]\n",
    "        else:\n",
    "            return self.seismic[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b08c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(cfg):\n",
    "    #First we extract the velocity and seismic data's for training and testing\n",
    "    vel_data = []; seismic_data = []; test_data = []\n",
    "\n",
    "    #Extracting and concatenating the training data\n",
    "    for domain in cfg.dset1: \n",
    "        model_path = Path(cfg.train_path) / domain / \"model\"\n",
    "        data_path = Path(cfg.train_path) / domain / \"data\"\n",
    "    \n",
    "        # Load all .npy files in this domain and extend the master lists\n",
    "        vel_data += [np.load(str(f)) for f in sorted(model_path.glob(\"*.npy\"))]\n",
    "        seismic_data += [np.load(str(f)) for f in sorted(data_path.glob(\"*.npy\"))]\n",
    "    \n",
    "    for domain in cfg.dset2: \n",
    "        model_path = Path(cfg.train_path) / domain \n",
    "        data_path = Path(cfg.train_path) / domain\n",
    "    \n",
    "        # Load all .npy files in this domain and extend the master lists\n",
    "        vel_data += [np.load(str(f)) for f in sorted(model_path.glob(\"vel*.npy\"))]\n",
    "        seismic_data += [np.load(str(f)) for f in sorted(data_path.glob(\"seis*.npy\"))]\n",
    "\n",
    "    \n",
    "    # Concatenate all at once\n",
    "    sample_points = sum(v.shape[0] for v in vel_data) #To ensure we have the required number of data\n",
    "    vel_data = np.concatenate(vel_data, axis=0)\n",
    "    vel_data = (vel_data - vel_data.mean())/(vel_data.std() + 1e-8)\n",
    "    seismic_data = np.concatenate(seismic_data, axis=0)\n",
    "    assert ( vel_data.shape[0] == sample_points and seismic_data.shape[0] == sample_points\n",
    "           ), f\"Expected sample size {sample_points} but got {vel_data.shape[0]} and {seismic_data.shape[0]}\"\n",
    "    print(f\"Training data --> Seismic: {seismic_data.shape}, Velocity: {vel_data.shape}\")\n",
    "\n",
    "    #We need to normalize (Z-score) our input before training\n",
    "    #s_mean = seismic_data.mean(axis=(0, 2, 3), keepdims=True); s_std = seismic_data.std(axis=(0, 2, 3), keepdims=True)\n",
    "    #seismic_data = (seismic_data - s_mean)/(s_std + 1e-6) #Epsilon is for stability\n",
    "\n",
    "    #Extracting the Test data\n",
    "    test_path = Path(cfg.test_path)\n",
    "    test_data += [np.load(str(f)) for f in sorted(test_path.glob(\"*.npy\"))[0:cfg.test_num]] #Only first few for illustration\n",
    "    test_sample_points = sum(v.shape[0] for v in test_data)\n",
    "    test_data = np.concatenate(test_data,axis=0)\n",
    "    test_data = (test_data - test_data.mean())/(test_data.std() + 1e-8)\n",
    "    test_data = np.expand_dims(test_data, axis=1)\n",
    "    test_data = np.repeat(test_data, 5, axis=1)\n",
    "    assert test_data.shape[0] == test_sample_points, f\"Expected test size {test_sample_points} but got {test_data.shape[0]} \"\n",
    "    print(f\"Testing data --> Seismic: {test_data.shape}\")\n",
    "\n",
    "    #Next, we take a portion of the train data for validation\n",
    "    X_train, X_val, y_train,y_val = train_test_split(\n",
    "        seismic_data, vel_data, test_size = 0.1, random_state = 42, shuffle = True\n",
    "    )\n",
    "    print(f\"After split --> X_train: {X_train.shape}, y_train: {y_train.shape} -- X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "    \n",
    "    #Loading the datasets into batches\n",
    "    train_dataset = SeismicDataset(X_train, y_train)\n",
    "    val_dataset = SeismicDataset(X_val, y_val)\n",
    "    test_dataset = SeismicDataset(test_data)\n",
    "\n",
    "    #DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size = cfg.batch_size, shuffle = True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = cfg.batch_size, shuffle = False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size = cfg.batch_size, shuffle = False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255810d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.test_loader = None\n",
    "\n",
    "    def setup(self, stage = None):\n",
    "        self.train_loader, self.val_loader, self.test_loader = prepare_data(self.cfg)\n",
    "        print('DataLoaded Successfully!')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "data_module = DataModule(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610dcb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data --> Seismic: (9000, 5, 1000, 70), Velocity: (9000, 1, 70, 70)\n",
      "Testing data --> Seismic: (65, 5, 1000, 70)\n",
      "After split --> X_train: (8100, 5, 1000, 70), y_train: (8100, 1, 70, 70) -- X_val: (900, 5, 1000, 70), y_val: (900, 1, 70, 70)\n",
      "DataLoaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b536af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test batch: <class 'torch.Tensor'> torch.Size([50, 5, 1000, 70])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(data_module.predict_dataloader()))\n",
    "print(\"Test batch:\", type(batch), batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef87f77",
   "metadata": {},
   "source": [
    "## Inversion Net Model\n",
    "\n",
    "As a summary, \n",
    "InversionNet is a deep learning model designed to learn a mapping from seismic reflection data to subsurface velocity models using an encoder-decoder convolutional neural network (CNN) coupled with a locally connected Conditional Random Field (CRF) for structure refinement. It has the same structure as the UNet but without the skip connections. Thus, allowing for different input and output feature space. (Unlike the UNet which requires that the input and output feature space are the same).\n",
    "\n",
    "The Network has the following architecture:\n",
    "1) The Input is a 2D seismic reflection data (competition dataset: 70 x 1000) with different sources. The output is a 2D subsurface velocity model (e.g., competition dataset shape =70x70)\n",
    "2) **Encoder**: Extract high-level features from the seismic data.\n",
    "    * Structure: \n",
    "        * Multiple convolution blocks (Conv + BatchNorm + ReLU).\n",
    "        * Early layers are 1D convolutions to capture temporal features.\n",
    "        * Later layers compress to a single high-dimensional vector.\n",
    "    *   No zero-padding in the last layer to reduce to a single feature vector.\n",
    "\n",
    "3) **Decoder**: Maps the encoded features to a velocity model.\n",
    "    * Structure: \n",
    "        * Mixed deconvolution and convolution blocks.\n",
    "        * Each deconv block is structured as:\n",
    "            * 4×4 transposed convolution with stride 2 for upsampling.\n",
    "            * 3×3 convolution for refinement.\n",
    "        * Final 1×1 convolution layer to map features to scalar velocity per pixel.\n",
    "        * Output is cropped to match the ground truth dimensions.\n",
    "    * The Loss function is the L1 Loss\n",
    "\n",
    "4) **Conditional Random Field (CRF)**: It refines the CNN's output by enforcing spatial consistency and capturing local interactions.\n",
    "    * CRF formulation:\n",
    "        * Unary potential: penalizes deviations from CNN predictions.\n",
    "        * Pairwise potential: enforces similarity between neighboring pixels.\n",
    "        * Neighborhood: fixed d×d window (e.g., d=20).\n",
    "        * Optimization via mean-field approximation.\n",
    "        * Only active during post-processing, not during CNN training.\n",
    "\n",
    "5) **The Training Setup**:\n",
    "        * Optimizer: Adam\n",
    "        * Batch size: 50\n",
    "        * Learning rate: starts at 0.0005, decays ×0.1 every 15 epochs\n",
    "        * Training: CNN for 30 epochs, then CRF refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a3f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
